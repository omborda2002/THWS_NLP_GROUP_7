# Google Tunix Hackathon - Competition Entry

## ğŸ† Competition

**Kaggle Competition:** [Google Tunix Hackathon](https://www.kaggle.com/competitions/google-tunix-hackathon)

## ğŸ“ Our Writeup

**Final Submission:** [View on Kaggle](https://www.kaggle.com/competitions/google-tunix-hackathon/writeups/new-writeup-1767829089071)

---

## ğŸ¯ Task

Fine-tune Google's Gemma model to output structured reasoning traces:

```
<reasoning>step-by-step thinking</reasoning>
<answer>final answer</answer>
```

## ğŸ› ï¸ Approach

| Component | Details |
|-----------|---------|
| **Base Model** | google/gemma-2-2b-it |
| **Method** | LoRA (Low-Rank Adaptation) |
| **Quantization** | 4-bit NF4 |
| **Training Data** | ~570k samples |
| **Training Time** | ~6 hours on A100 40GB |

## ğŸ“Š Datasets Used

- GSM8K (Math word problems)
- OpenThoughts-114k (R1 distilled reasoning)
- Bespoke-Stratos-17k (High quality R1)
- Medical-O1 (Medical reasoning)
- MetaMathQA (Augmented math)

## ğŸ“ Project Structure

```
tunix_hack/
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ TASK_SUMMARY.md        # Detailed task summary
â””â”€â”€ notebooks/
    â”œâ”€â”€ train_a100.py      # Training script (A100)
    â”œâ”€â”€ train_kaggle_fixed.py
    â”œâ”€â”€ tunixhack.ipynb    # Main Kaggle notebook
    â”œâ”€â”€ inference.py       # Inference script
    â””â”€â”€ gemma-reasoning/   # Trained LoRA adapter
```

## ğŸ‘¥ Team Contributions

All team members contributed equally to this competition.
