{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Gemma Reasoning Model - Inference\n",
    "\n",
    "**Author:** Om Borda (omborda2002)  \n",
    "**Competition:** Google Tunix Hack  \n",
    "**Model:** Gemma 2B IT + LoRA Fine-tuned on 570k reasoning samples\n",
    "\n",
    "## Output Format\n",
    "```\n",
    "<reasoning>step-by-step thinking</reasoning>\n",
    "<answer>final answer</answer>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths - UPDATE THIS to your Kaggle model path\n",
    "LORA_PATH = \"/kaggle/input/gemma-reasoning-lora\"  # Your uploaded LoRA adapter\n",
    "BASE_MODEL = \"google/gemma-2-2b-it\"\n",
    "\n",
    "# Check if path exists\n",
    "if os.path.exists(LORA_PATH):\n",
    "    print(f\"‚úì LoRA adapter found at: {LORA_PATH}\")\n",
    "    print(f\"  Contents: {os.listdir(LORA_PATH)}\")\n",
    "else:\n",
    "    print(f\"‚úó LoRA path not found: {LORA_PATH}\")\n",
    "    print(\"  Make sure to add your model as input data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Get HF token from Kaggle secrets\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "    print(\"‚úì HuggingFace token found\")\n",
    "except:\n",
    "    try:\n",
    "        hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "        print(\"‚úì HuggingFace token found\")\n",
    "    except:\n",
    "        hf_token = None\n",
    "        print(\"‚ö† No HF token - may fail for gated models\")\n",
    "\n",
    "# Load tokenizer from HuggingFace (not from LoRA path)\n",
    "print(\"Loading tokenizer from HuggingFace...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, token=hf_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"‚úì Tokenizer loaded\")\n",
    "\n",
    "# Load base model\n",
    "print(\"\\nLoading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=hf_token,\n",
    ")\n",
    "print(\"‚úì Base model loaded\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "print(\"Loading LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(base_model, LORA_PATH)\n",
    "model.eval()\n",
    "print(\"‚úì Model ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question, max_tokens=500, temperature=0.7):\n",
    "    \"\"\"Generate a response with reasoning.\"\"\"\n",
    "    prompt = f\"<start_of_turn>user\\n{question}\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract model response only\n",
    "    if \"<start_of_turn>model\" in response:\n",
    "        response = response.split(\"<start_of_turn>model\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"‚úì Inference function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample test questions\n",
    "test_questions = [\n",
    "    \"What is 125 + 347?\",\n",
    "    \"Solve: 2x + 5 = 13\",\n",
    "    \"A train travels 240 km in 4 hours. What is its speed?\",\n",
    "    \"What is the probability of rolling a 6 on a fair die?\",\n",
    "    \"If all cats are animals, and Whiskers is a cat, what can we conclude?\",\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üß™ MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"\\nüìù Question: {q}\")\n",
    "    print(\"-\"*60)\n",
    "    response = generate_response(q)\n",
    "    print(f\"ü§ñ Response:\\n{response}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition Submission\n",
    "\n",
    "For actual competition submission, load the test data and generate predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and modify for actual competition submission\n",
    "# import pandas as pd\n",
    "# \n",
    "# # Load test data\n",
    "# test_df = pd.read_csv(\"/kaggle/input/competition-test-data/test.csv\")\n",
    "# \n",
    "# # Generate predictions\n",
    "# predictions = []\n",
    "# for idx, row in test_df.iterrows():\n",
    "#     question = row['question']  # adjust column name as needed\n",
    "#     response = generate_response(question)\n",
    "#     predictions.append(response)\n",
    "#     \n",
    "#     if idx % 10 == 0:\n",
    "#         print(f\"Processed {idx+1}/{len(test_df)}\")\n",
    "# \n",
    "# # Save submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': test_df['id'],\n",
    "#     'prediction': predictions\n",
    "# })\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "# print(\"‚úì Submission saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Model:** Gemma 2B IT + LoRA  \n",
    "**Training Data:** ~570k reasoning samples  \n",
    "- GSM8K, OpenThoughts, Stratos, Medical-O1, MetaMathQA\n",
    "\n",
    "**Output Format:**\n",
    "```\n",
    "<reasoning>step-by-step thinking</reasoning>\n",
    "<answer>final answer</answer>\n",
    "```\n",
    "\n",
    "**Author:** Om Borda (omborda2002)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
