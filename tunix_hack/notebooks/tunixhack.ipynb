{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":85979,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72240,"modelId":76277}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ§  Google Tunix Hack - Reasoning Model Training\n\n**Author:** Om Borda (omborda2002)  \n**Competition:** Google Tunix Hack  \n**Model:** Gemma 2B  \n\n## Output Format\n```\n<reasoning>step-by-step thinking</reasoning>\n<answer>final answer</answer>\n```\n\n## Datasets (~250k samples)\n- GSM8K (7.4k) - Math word problems\n- OpenThoughts-114k (100k) - R1 distilled reasoning\n- Bespoke-Stratos-17k (17k) - High quality R1\n- Medical-O1 (44k) - Medical reasoning\n- MetaMathQA (80k) - Augmented math","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers datasets accelerate bitsandbytes peft trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:13:28.035446Z","iopub.execute_input":"2026-01-06T16:13:28.035782Z","iopub.status.idle":"2026-01-06T16:13:31.460675Z","shell.execute_reply.started":"2026-01-06T16:13:28.035753Z","shell.execute_reply":"2026-01-06T16:13:31.459882Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import load_dataset, Dataset\nimport random\nimport re\nimport os\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:13:31.462359Z","iopub.execute_input":"2026-01-06T16:13:31.462657Z","iopub.status.idle":"2026-01-06T16:13:31.468681Z","shell.execute_reply.started":"2026-01-06T16:13:31.462598Z","shell.execute_reply":"2026-01-06T16:13:31.468035Z"}},"outputs":[{"name":"stdout","text":"PyTorch: 2.8.0+cu126\nCUDA: True\nGPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## 2. Configuration","metadata":{}},{"cell_type":"code","source":"# Detect Kaggle environment\nIS_KAGGLE = os.path.exists('/kaggle')\n\nCONFIG = {\n    # Model path (Kaggle hub or HuggingFace)\n    \"model_name\": \"/kaggle/input/gemma-2/transformers/gemma-2-2b-it/1\" if IS_KAGGLE else \"google/gemma-2-2b-it\",\n    \"max_seq_length\": 1024,\n    \n    # LoRA\n    \"lora_r\": 32,\n    \"lora_alpha\": 64,\n    \"lora_dropout\": 0.05,\n    \n    # Training\n    \"batch_size\": 4,\n    \"gradient_accumulation_steps\": 8,\n    \"learning_rate\": 2e-4,\n    \"num_epochs\": 1,\n    \"warmup_ratio\": 0.03,\n    \n    # Output\n    \"output_dir\": \"/kaggle/working/gemma-reasoning\" if IS_KAGGLE else \"./gemma-reasoning\",\n}\n\n# Dataset limits\nDATASET_LIMITS = {\n    \"gsm8k\": None,           # All 7.4k\n    \"openthoughts\": None,   # 100k from 114k\n    \"stratos\": None,          # All 17k\n    \"medical_o1\": None,       # All ~44k\n    \"metamath\": None,        # 80k from 395k\n}\n\nprint(f\"Running on: {'Kaggle' if IS_KAGGLE else 'Local'}\")\nprint(f\"Model: {CONFIG['model_name']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:13:31.469595Z","iopub.execute_input":"2026-01-06T16:13:31.469914Z","iopub.status.idle":"2026-01-06T16:13:31.487446Z","shell.execute_reply.started":"2026-01-06T16:13:31.469892Z","shell.execute_reply":"2026-01-06T16:13:31.486808Z"}},"outputs":[{"name":"stdout","text":"Running on: Kaggle\nModel: /kaggle/input/gemma-2/transformers/gemma-2-2b-it/1\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 3. Data Formatters","metadata":{}},{"cell_type":"code","source":"def extract_think_answer(text):\n    \"\"\"Extract thinking from <think> tags.\"\"\"\n    think_match = re.search(r'<think>(.*?)</think>', text, re.DOTALL)\n    thinking = think_match.group(1).strip() if think_match else \"\"\n    if '</think>' in text:\n        answer = text.split('</think>')[-1].strip()\n    else:\n        answer = text\n    return thinking, answer\n\ndef format_gsm8k(example):\n    \"\"\"Format GSM8K math problems.\"\"\"\n    question = example.get('question', '')\n    answer_text = example.get('answer', '')\n    \n    if '####' in answer_text:\n        reasoning = answer_text.split('####')[0].strip()\n        final = answer_text.split('####')[1].strip()\n    else:\n        reasoning = answer_text\n        final = answer_text.split('\\n')[-1]\n    \n    return {\n        \"instruction\": question,\n        \"response\": f\"<reasoning>\\n{reasoning}\\n</reasoning>\\n<answer>{final}</answer>\"\n    }\n\ndef format_openthoughts(example):\n    \"\"\"Format OpenThoughts-114k.\"\"\"\n    try:\n        conversations = example.get('conversations', [])\n        question, answer = \"\", \"\"\n        \n        for conv in conversations:\n            role = conv.get('from', '')\n            if role in ['human', 'user']:\n                question = conv.get('value', '')\n            elif role in ['gpt', 'assistant']:\n                answer = conv.get('value', '')\n        \n        if not question or not answer:\n            return None\n        \n        thinking, final = extract_think_answer(answer)\n        if thinking:\n            response = f\"<reasoning>\\n{thinking}\\n</reasoning>\\n<answer>{final}</answer>\"\n        else:\n            response = f\"<reasoning>\\n{answer[:1500]}\\n</reasoning>\\n<answer>{answer[-300:]}</answer>\"\n        \n        return {\"instruction\": question, \"response\": response}\n    except:\n        return None\n\ndef format_stratos(example):\n    \"\"\"Format Bespoke-Stratos-17k.\"\"\"\n    try:\n        conversations = example.get('conversations', [])\n        question, answer = \"\", \"\"\n        \n        for conv in conversations:\n            role = conv.get('from', '')\n            if role in ['human', 'user']:\n                question = conv.get('value', '')\n            elif role in ['gpt', 'assistant']:\n                answer = conv.get('value', '')\n        \n        if not question or not answer:\n            return None\n        \n        thinking, final = extract_think_answer(answer)\n        if thinking:\n            response = f\"<reasoning>\\n{thinking}\\n</reasoning>\\n<answer>{final}</answer>\"\n        else:\n            response = f\"<reasoning>\\n{answer[:1500]}\\n</reasoning>\\n<answer>{answer[-300:]}</answer>\"\n        \n        return {\"instruction\": question, \"response\": response}\n    except:\n        return None\n\ndef format_medical_o1(example):\n    \"\"\"Format Medical O1 reasoning.\"\"\"\n    try:\n        question = example.get('Question', '')\n        cot = example.get('Complex_CoT', '')\n        response_text = example.get('Response', '')\n        \n        if not question:\n            return None\n        \n        if cot:\n            response = f\"<reasoning>\\n{cot[:1500]}\\n</reasoning>\\n<answer>{response_text}</answer>\"\n        else:\n            response = f\"<reasoning>\\nAnalyzing medical question.\\n</reasoning>\\n<answer>{response_text}</answer>\"\n        \n        return {\"instruction\": question, \"response\": response}\n    except:\n        return None\n\ndef format_metamath(example):\n    \"\"\"Format MetaMathQA.\"\"\"\n    query = example.get('query', '')\n    response = example.get('response', '')\n    \n    if 'The answer is' in response:\n        parts = response.split('The answer is')\n        reasoning = parts[0].strip()\n        final = parts[1].strip().rstrip('.')\n    else:\n        reasoning = response\n        final = response.split('\\n')[-1]\n    \n    return {\n        \"instruction\": query,\n        \"response\": f\"<reasoning>\\n{reasoning[:1500]}\\n</reasoning>\\n<answer>{final}</answer>\"\n    }\n\nprint(\"âœ“ Formatters ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:13:31.489010Z","iopub.execute_input":"2026-01-06T16:13:31.489226Z","iopub.status.idle":"2026-01-06T16:13:31.505904Z","shell.execute_reply.started":"2026-01-06T16:13:31.489205Z","shell.execute_reply":"2026-01-06T16:13:31.505193Z"}},"outputs":[{"name":"stdout","text":"âœ“ Formatters ready\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## 4. Gemma Chat Template","metadata":{}},{"cell_type":"code","source":"def create_prompt(instruction: str) -> str:\n    \"\"\"Gemma chat format.\"\"\"\n    return f\"<start_of_turn>user\\n{instruction}\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n\ndef format_for_training(example: dict) -> dict:\n    \"\"\"Final training format.\"\"\"\n    if example is None:\n        return None\n    prompt = create_prompt(example[\"instruction\"])\n    return {\"text\": prompt + example[\"response\"] + \"<end_of_turn>\"}\n\n# Example\nsample = format_for_training({\"instruction\": \"What is 2+2?\", \"response\": \"<reasoning>\\n2+2=4\\n</reasoning>\\n<answer>4</answer>\"})\nprint(\"Sample format:\")\nprint(sample[\"text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:13:31.506910Z","iopub.execute_input":"2026-01-06T16:13:31.507142Z","iopub.status.idle":"2026-01-06T16:13:31.521667Z","shell.execute_reply.started":"2026-01-06T16:13:31.507122Z","shell.execute_reply":"2026-01-06T16:13:31.521008Z"}},"outputs":[{"name":"stdout","text":"Sample format:\n<start_of_turn>user\nWhat is 2+2?\n<end_of_turn>\n<start_of_turn>model\n<reasoning>\n2+2=4\n</reasoning>\n<answer>4</answer><end_of_turn>\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## 5. Load Datasets","metadata":{}},{"cell_type":"code","source":"def load_and_format_dataset(name, config, formatter, limit, desc):\n    \"\"\"Load and format a single dataset.\"\"\"\n    print(f\"\\nðŸ“Š Loading {desc}...\")\n    try:\n        if config:\n            ds = load_dataset(name, config, split=\"train\")\n        else:\n            ds = load_dataset(name, split=\"train\")\n        \n        if limit and len(ds) > limit:\n            ds = ds.shuffle(seed=42).select(range(limit))\n        \n        formatted = [formatter(ex) for ex in ds]\n        formatted = [f for f in formatted if f is not None]\n        \n        print(f\"   âœ“ {len(formatted):,} examples\")\n        return formatted\n    except Exception as e:\n        print(f\"   âœ— Failed: {str(e)[:50]}\")\n        return []\n\n# Load all datasets\nall_examples = []\n\nall_examples += load_and_format_dataset(\"gsm8k\", \"main\", format_gsm8k, DATASET_LIMITS[\"gsm8k\"], \"GSM8K\")\nall_examples += load_and_format_dataset(\"open-thoughts/OpenThoughts-114k\", None, format_openthoughts, DATASET_LIMITS[\"openthoughts\"], \"OpenThoughts\")\nall_examples += load_and_format_dataset(\"bespokelabs/Bespoke-Stratos-17k\", None, format_stratos, DATASET_LIMITS[\"stratos\"], \"Stratos\")\nall_examples += load_and_format_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", format_medical_o1, DATASET_LIMITS[\"medical_o1\"], \"Medical-O1-en\")\nall_examples += load_and_format_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en_mix\", format_medical_o1, DATASET_LIMITS[\"medical_o1\"], \"Medical-O1-mix\")\nall_examples += load_and_format_dataset(\"meta-math/MetaMathQA\", None, format_metamath, DATASET_LIMITS[\"metamath\"], \"MetaMath\")\n\nprint(f\"\\nðŸ“Š Total collected: {len(all_examples):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:13:31.522419Z","iopub.execute_input":"2026-01-06T16:13:31.522765Z","iopub.status.idle":"2026-01-06T16:14:58.121235Z","shell.execute_reply.started":"2026-01-06T16:13:31.522700Z","shell.execute_reply":"2026-01-06T16:14:58.120533Z"}},"outputs":[{"name":"stdout","text":"\nðŸ“Š Loading GSM8K...\n   âœ“ 7,473 examples\n\nðŸ“Š Loading OpenThoughts...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/113957 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8048088215b64ef4bb27096b87368100"}},"metadata":{}},{"name":"stdout","text":"   âœ“ 113,957 examples\n\nðŸ“Š Loading Stratos...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c9689ee43cb42fb84cc035dbf285abf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/125M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfe1a0799a7e40208c7ac7c25b3474d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/16710 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebfa283eab4e48c3b418a6c16715f26a"}},"metadata":{}},{"name":"stdout","text":"   âœ“ 16,710 examples\n\nðŸ“Š Loading Medical-O1-en...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94e53713b89049e89f6609b46a76c254"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"medical_o1_sft.json:   0%|          | 0.00/58.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"572ecf6296c54eaeac747c7d9fdcd79d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/19704 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1dd6ce3a25e4feca0afc02839cdc2a8"}},"metadata":{}},{"name":"stdout","text":"   âœ“ 19,704 examples\n\nðŸ“Š Loading Medical-O1-mix...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"medical_o1_sft_mix.json:   0%|          | 0.00/73.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a85d006e3c154f4483996baabfce4cf0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/24887 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"865df8388410423d967cd0dfd1f4953f"}},"metadata":{}},{"name":"stdout","text":"   âœ“ 24,887 examples\n\nðŸ“Š Loading MetaMath...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb334198c9cd4e3e893955fbb54999b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"MetaMathQA-395K.json:   0%|          | 0.00/396M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b52c7ba3f174f4b8c0ba771f69febba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/395000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8db70b3d93e84648aac0f5d9e7f5451e"}},"metadata":{}},{"name":"stdout","text":"   âœ“ 395,000 examples\n\nðŸ“Š Total collected: 577,731\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Shuffle and prepare final dataset\nrandom.seed(42)\nrandom.shuffle(all_examples)\n\n# Filter valid examples\nvalid = []\nfor ex in all_examples:\n    if ex and len(ex.get(\"instruction\", \"\")) > 10 and len(ex.get(\"response\", \"\")) > 30:\n        if \"<reasoning>\" in ex[\"response\"] and \"<answer>\" in ex[\"response\"]:\n            valid.append(ex)\n\n# Format for training\nfinal_data = [format_for_training(ex) for ex in valid]\nfinal_data = [f for f in final_data if f and len(f[\"text\"]) < 4000]  # Skip very long\n\ndataset = Dataset.from_list(final_data)\nprint(f\"\\nâœ… Final training dataset: {len(dataset):,} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:14:58.122139Z","iopub.execute_input":"2026-01-06T16:14:58.122431Z","iopub.status.idle":"2026-01-06T16:15:07.944963Z","shell.execute_reply.started":"2026-01-06T16:14:58.122403Z","shell.execute_reply":"2026-01-06T16:15:07.944338Z"}},"outputs":[{"name":"stdout","text":"\nâœ… Final training dataset: 570,699 samples\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Preview sample\nprint(\"ðŸ“ Sample training example:\")\nprint(\"=\"*60)\nprint(dataset[0][\"text\"][:800])\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:15:07.945755Z","iopub.execute_input":"2026-01-06T16:15:07.945964Z","iopub.status.idle":"2026-01-06T16:15:07.951289Z","shell.execute_reply.started":"2026-01-06T16:15:07.945938Z","shell.execute_reply":"2026-01-06T16:15:07.950578Z"}},"outputs":[{"name":"stdout","text":"ðŸ“ Sample training example:\n============================================================\n<start_of_turn>user\nWhat is $ 6 \\div 3 - 2 - X + 2 \\cdot 8$?\nIf we know the answer to the above question is 8, what is the value of unknown variable X?\n<end_of_turn>\n<start_of_turn>model\n<reasoning>\nWe want to find the value of $X$ in the given expression.\nUsing the order of operations (PEMDAS), we can simplify the expression:\n$6 \\div 3 - 2 - X + 2 \\cdot 8$\nFirst, we perform the multiplication:\n$6 \\div 3 - 2 - X + 16$\nNext, we perform the division:\n$2 - 2 - X + 16$\nThen, we perform the subtraction:\n$0 - X + 16$\nFinally, we perform the addition:\n$-X + 16$\nWe are given that the value of the expression is 8, so we can write:\n$-X + 16 = 8$\nTo solve for $X$, we can subtract 16 from both sides of the equation:\n$-X = 8 - 16$\n$-X = -8$\nDividing both sides of the equation by -1, we find:\n$X = 8$\nTh\n============================================================\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## 6. Load Model","metadata":{}},{"cell_type":"code","source":"# Quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel_path = CONFIG['model_name']\nprint(f\"Model path: {model_path}\")\n\n# Get HuggingFace token from Kaggle secrets\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\ntry:\n    hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n    print(\"âœ“ HuggingFace token found\")\nexcept:\n    try:\n        hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n        print(\"âœ“ HuggingFace token found\")\n    except:\n        raise ValueError(\"Please add HUGGINGFACE_TOKEN or HF_TOKEN to Kaggle Secrets\")\n\n# For Kaggle, load from HuggingFace Hub with authentication\nif IS_KAGGLE:\n    print(\"Loading from HuggingFace Hub with authentication...\")\n    model_name = \"google/gemma-2-2b-it\"\nelse:\n    model_name = model_path\n    hf_token = None\n\n# Load tokenizer\nprint(f\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\nprint(f\"âœ“ Tokenizer loaded\")\n\n# Load model\nprint(f\"Loading model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    token=hf_token,\n)\n\nmodel = prepare_model_for_kbit_training(model)\nprint(\"âœ“ Model loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:15:07.952226Z","iopub.execute_input":"2026-01-06T16:15:07.952576Z","iopub.status.idle":"2026-01-06T16:15:43.443532Z","shell.execute_reply.started":"2026-01-06T16:15:07.952543Z","shell.execute_reply":"2026-01-06T16:15:43.442546Z"}},"outputs":[{"name":"stdout","text":"Model path: /kaggle/input/gemma-2/transformers/gemma-2-2b-it/1\nâœ“ HuggingFace token found\nLoading from HuggingFace Hub with authentication...\nLoading tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4345d01ce35244adae2ec580982db346"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"749ed1c2deef4457a75e451e983167aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26de06d1ee604207ae143f5489c8eb8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b9674717a3946028bb7ae814f94e0fc"}},"metadata":{}},{"name":"stdout","text":"âœ“ Tokenizer loaded\nLoading model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84305a991d94452b9fc95331e082b55a"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c608bca513a4dbfaec10875219f1bfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7edb7972cff2440ba9916dc19d6b6fad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f96361cde9004ddf8077f8c9b8fe6858"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66824557f93d490b98b4393759a75505"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aded1a7c224490cab5bda762dffc8b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59ab4bd277fc4bdca0db18adc6c7b5f1"}},"metadata":{}},{"name":"stdout","text":"âœ“ Model loaded\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## 7. Apply LoRA","metadata":{}},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=CONFIG[\"lora_r\"],\n    lora_alpha=CONFIG[\"lora_alpha\"],\n    lora_dropout=CONFIG[\"lora_dropout\"],\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:15:43.446435Z","iopub.execute_input":"2026-01-06T16:15:43.447121Z","iopub.status.idle":"2026-01-06T16:15:44.032335Z","shell.execute_reply.started":"2026-01-06T16:15:43.447093Z","shell.execute_reply":"2026-01-06T16:15:44.031709Z"}},"outputs":[{"name":"stdout","text":"trainable params: 41,533,440 || all params: 2,655,875,328 || trainable%: 1.5638\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## 8. Training","metadata":{}},{"cell_type":"code","source":"# Training config\ntraining_args = SFTConfig(\n    output_dir=CONFIG[\"output_dir\"],\n    num_train_epochs=CONFIG[\"num_epochs\"],\n    per_device_train_batch_size=CONFIG[\"batch_size\"],\n    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n    learning_rate=CONFIG[\"learning_rate\"],\n    warmup_ratio=CONFIG[\"warmup_ratio\"],\n    logging_steps=50,\n    save_strategy=\"steps\",\n    save_steps=500,\n    save_total_limit=2,\n    bf16=True,\n    optim=\"paged_adamw_32bit\",\n    lr_scheduler_type=\"cosine\",\n    report_to=\"none\",\n    gradient_checkpointing=True,\n    max_length=CONFIG[\"max_seq_length\"],\n)\n\n# Initialize trainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n    args=training_args,\n)\n\n# Estimate\nsteps = len(dataset) // (CONFIG[\"batch_size\"] * CONFIG[\"gradient_accumulation_steps\"])\nprint(f\"\\nðŸš€ Training Plan:\")\nprint(f\"   Samples: {len(dataset):,}\")\nprint(f\"   Steps: ~{steps:,}\")\nprint(f\"   Estimated time: ~4-6 hours\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:15:44.033316Z","iopub.execute_input":"2026-01-06T16:15:44.033625Z","iopub.status.idle":"2026-01-06T16:24:25.637251Z","shell.execute_reply.started":"2026-01-06T16:15:44.033591Z","shell.execute_reply":"2026-01-06T16:24:25.636616Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/570699 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8ffa41fa95f471aa364095d68ca4193"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/570699 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60bc94cd16394f9c838cd29c96e59f96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/570699 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad642fcc073c4768b0f01ca6ab7d82a2"}},"metadata":{}},{"name":"stdout","text":"\nðŸš€ Training Plan:\n   Samples: 570,699\n   Steps: ~17,834\n   Estimated time: ~4-6 hours\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Train!\nprint(\"\\n\" + \"=\"*60)\nprint(\"ðŸš€ STARTING TRAINING\")\nprint(\"=\"*60)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:24:25.638288Z","iopub.execute_input":"2026-01-06T16:24:25.638580Z","iopub.status.idle":"2026-01-06T16:24:37.168777Z","shell.execute_reply.started":"2026-01-06T16:24:25.638550Z","shell.execute_reply":"2026-01-06T16:24:37.167689Z"}},"outputs":[{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'pad_token_id': 1}.\n","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nðŸš€ STARTING TRAINING\n============================================================\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3205845401.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_activation_offload_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1245\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4019\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4020\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4022\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_token_scaling\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"dft\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m         (loss, outputs) = super().compute_loss(\n\u001b[0m\u001b[1;32m   1154\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4108\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4109\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4110\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4111\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4112\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1848\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1851\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gemma2/modeling_gemma2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         return CausalLMOutputWithPast(\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36mForCausalLMLoss\u001b[0;34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, shift_labels, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Enable model parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mshift_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfixed_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36mfixed_cross_entropy\u001b[0;34m(source, target, num_items_in_batch, ignore_index, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m ) -> torch.Tensor:\n\u001b[1;32m     35\u001b[0m     \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sum\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# just in case users pass an int for num_items_in_batch, which could be the case for custom trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3460\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3461\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3462\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3463\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3464\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.25 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.92 GiB is free. Process 4591 has 13.97 GiB memory in use. Of the allocated memory 9.53 GiB is allocated by PyTorch, and 4.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 2.25 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.92 GiB is free. Process 4591 has 13.97 GiB memory in use. Of the allocated memory 9.53 GiB is allocated by PyTorch, and 4.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":24},{"cell_type":"code","source":"# Save model\nprint(\"\\nðŸ’¾ Saving model...\")\ntrainer.save_model()\ntokenizer.save_pretrained(CONFIG[\"output_dir\"])\nprint(f\"âœ… Model saved to {CONFIG['output_dir']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:24:37.169656Z","iopub.status.idle":"2026-01-06T16:24:37.169937Z","shell.execute_reply.started":"2026-01-06T16:24:37.169810Z","shell.execute_reply":"2026-01-06T16:24:37.169826Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Test Model","metadata":{}},{"cell_type":"code","source":"def generate_response(question, max_tokens=400):\n    \"\"\"Generate a response with reasoning.\"\"\"\n    prompt = create_prompt(question)\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    model.eval()\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            temperature=0.7,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if \"<start_of_turn>model\" in response:\n        response = response.split(\"<start_of_turn>model\")[-1]\n    return response.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:24:37.170738Z","iopub.status.idle":"2026-01-06T16:24:37.171085Z","shell.execute_reply.started":"2026-01-06T16:24:37.170909Z","shell.execute_reply":"2026-01-06T16:24:37.170931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test questions\ntest_questions = [\n    \"What is 125 + 347?\",\n    \"Solve: 2x + 5 = 13\",\n    \"A train travels 240 km in 4 hours. What is its speed?\",\n    \"What is the probability of rolling a 6 on a fair die?\",\n    \"What are the symptoms of diabetes?\",\n]\n\nprint(\"=\"*60)\nprint(\"ðŸ§ª MODEL EVALUATION\")\nprint(\"=\"*60)\n\nfor q in test_questions:\n    print(f\"\\nðŸ“ Question: {q}\")\n    print(f\"ðŸ¤– Response:\\n{generate_response(q)}\")\n    print(\"-\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:24:37.171967Z","iopub.status.idle":"2026-01-06T16:24:37.172190Z","shell.execute_reply.started":"2026-01-06T16:24:37.172081Z","shell.execute_reply":"2026-01-06T16:24:37.172095Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. Summary\n\n### Training Complete! âœ…\n\n**Model:** Gemma 2B fine-tuned with LoRA\n\n**Output Format:**\n```\n<reasoning>step-by-step thinking</reasoning>\n<answer>final answer</answer>\n```\n\n**Datasets Used:**\n- GSM8K - Math word problems\n- OpenThoughts - R1 distilled reasoning\n- Bespoke-Stratos - High quality reasoning\n- Medical-O1 - Medical reasoning\n- MetaMathQA - Augmented math\n\n**Hyperparameters:**\n- LoRA rank: 32, alpha: 64\n- Learning rate: 2e-4\n- Batch size: 4 Ã— 8 = 32 effective\n- 1 epoch over ~250k samples","metadata":{}}]}